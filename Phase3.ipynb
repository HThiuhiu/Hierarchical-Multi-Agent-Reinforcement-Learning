{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a989ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, agent_state_dim, target_feature_dim, embed_dim=128, num_heads=4, distance_mode='log'):\n",
    "        super(Attention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.distance_mode = distance_mode\n",
    "        \n",
    "        self.agent_encoder = nn.Sequential(\n",
    "            nn.Linear(agent_state_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Linear(target_feature_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.agent_self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm_agent = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Input cho lớp quyết định: Agent feat + Target feat + Distance\n",
    "        self.assignment_attention = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2 + 1, 256), \n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, agent_states, global_targets, assignment_mode='gumbel', temperature=1.0):\n",
    "        batch_size, num_agents, _ = agent_states.size()\n",
    "        _, num_targets, _ = global_targets.size()\n",
    "\n",
    "        agent_emb = self.agent_encoder(agent_states)      # [B, N, Emb]\n",
    "        target_emb = self.target_encoder(global_targets)  # [B, M, Emb]\n",
    "\n",
    "        # Self-attention giữa các agents\n",
    "        agent_context, _ = self.agent_self_attn(query=agent_emb, key=agent_emb, value=agent_emb)\n",
    "        agent_features = self.norm_agent(agent_emb + agent_context)\n",
    "        \n",
    "        # Tính khoảng cách\n",
    "        a_pos = agent_states[:, :, :2].unsqueeze(2) # [B, N, 1, 2]\n",
    "        t_pos = global_targets[:, :, :2].unsqueeze(1) # [B, 1, M, 2]\n",
    "        dist_vec = a_pos - t_pos \n",
    "        dist = torch.norm(dist_vec, dim=-1, keepdim=True) # [B, N, M, 1]\n",
    "        \n",
    "        if self.distance_mode == 'log':\n",
    "            dist_scalar = torch.log(dist + 1e-6)\n",
    "        else:\n",
    "            dist_scalar = dist\n",
    "\n",
    "        # Expand dimensions để ghép nối (Concatenate)\n",
    "        agent_features_exp = agent_features.unsqueeze(2).expand(-1, -1, num_targets, -1)\n",
    "        target_emb_exp = target_emb.unsqueeze(1).expand(-1, num_agents, -1, -1)\n",
    "\n",
    "        combined = torch.cat([agent_features_exp, target_emb_exp, dist_scalar], dim=-1)\n",
    "\n",
    "        logits = self.assignment_attention(combined).squeeze(-1) # [B, N, M]\n",
    "        \n",
    "        # --- SỬA LỖI LOGIC TẠI ĐÂY ---\n",
    "        if assignment_mode == 'gumbel':\n",
    "            if self.training:\n",
    "                # Hard=True trả về one-hot nhưng có gradient cho backprop\n",
    "                weights = F.gumbel_softmax(logits, tau=temperature, hard=True, dim=-1)\n",
    "            else:\n",
    "                indices = torch.argmax(logits, dim=-1)\n",
    "                weights = F.one_hot(indices, num_classes=num_targets).float()\n",
    "        else:\n",
    "            weights = torch.sigmoid(logits) # Soft probability\n",
    "\n",
    "        return weights # Đảm bảo dòng này nằm ngoài cùng, không thuộc if/else nào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d885241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimulationEnv:\n",
    "    def __init__(self, num_agents=4, num_targets=5, area_size=100.0, device='cpu'):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_targets = num_targets\n",
    "        self.area_size = area_size\n",
    "        self.device = device\n",
    "        self.max_speed_agent = 2.0\n",
    "        self.max_speed_target = 1.0\n",
    "        \n",
    "        # Ngưỡng bao phủ (Agent cần đến gần Target bao nhiêu mét thì tính là cover)\n",
    "        self.coverage_radius = 10.0 \n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Khởi tạo vị trí ngẫu nhiên [x, y]\n",
    "        self.agent_pos = torch.rand(1, self.num_agents, 2).to(self.device) * self.area_size\n",
    "        self.target_pos = torch.rand(1, self.num_targets, 2).to(self.device) * self.area_size\n",
    "        \n",
    "        # Vận tốc target ngẫu nhiên\n",
    "        self.target_vel = (torch.rand(1, self.num_targets, 2).to(self.device) - 0.5) * 2 * self.max_speed_target\n",
    "        \n",
    "        # Dummy features cho camera (pan, tilt, zoom) để khớp input model\n",
    "        self.agent_ptz = torch.zeros(1, self.num_agents, 3).to(self.device)\n",
    "        \n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Gom lại thành tensor đúng shape model yêu cầu\n",
    "        # agent_states: (Batch, N, 5) -> [x, y, pan, tilt, zoom]\n",
    "        agent_states = torch.cat([self.agent_pos, self.agent_ptz], dim=-1)\n",
    "        \n",
    "        # global_targets: (Batch, M, 4) -> [x, y, vx, vy]\n",
    "        global_targets = torch.cat([self.target_pos, self.target_vel], dim=-1)\n",
    "        \n",
    "        return agent_states, global_targets\n",
    "\n",
    "    def step(self, assignment_weights):\n",
    "        \"\"\"\n",
    "        assignment_weights: (Batch, N, M) - Output từ Gumbel Softmax\n",
    "        \"\"\"\n",
    "        # 1. Di chuyển Targets (Bounce off walls - dội ngược khi gặp tường)\n",
    "        self.target_pos += self.target_vel\n",
    "        # Kiểm tra biên và đảo chiều vận tốc nếu chạm tường\n",
    "        for i in range(2): # x and y\n",
    "            mask_lower = self.target_pos[:, :, i] < 0\n",
    "            mask_upper = self.target_pos[:, :, i] > self.area_size\n",
    "            self.target_vel[:, :, i][mask_lower | mask_upper] *= -1\n",
    "            self.target_pos[:, :, i] = torch.clamp(self.target_pos[:, :, i], 0, self.area_size)\n",
    "\n",
    "        # 2. Di chuyển Agents dựa trên Assignment\n",
    "        # Model chọn target nào (dựa trên trọng số lớn nhất), Agent sẽ đi về hướng đó\n",
    "        # assignment_weights shape: (1, N, M)\n",
    "        # target_pos shape: (1, M, 2)\n",
    "        \n",
    "        # Tính vị trí mục tiêu mong muốn cho mỗi agent (Weighted Average Position)\n",
    "        # Vì Gumbel hard=True nên nó sẽ là vị trí của đúng 1 target được chọn\n",
    "        target_destinations = torch.bmm(assignment_weights, self.target_pos) # (1, N, 2)\n",
    "        \n",
    "        # Vector hướng di chuyển\n",
    "        direction = target_destinations - self.agent_pos\n",
    "        distance = torch.norm(direction, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Chuẩn hóa vector hướng và nhân với tốc độ tối đa\n",
    "        move_step = (direction / (distance + 1e-6)) * self.max_speed_agent\n",
    "        \n",
    "        # Nếu đã ở rất gần target, không cần đi hết tốc độ (tránh rung lắc)\n",
    "        move_step = torch.where(distance < self.max_speed_agent, direction, move_step)\n",
    "        \n",
    "        self.agent_pos += move_step\n",
    "        self.agent_pos = torch.clamp(self.agent_pos, 0, self.area_size)\n",
    "\n",
    "        # 3. Tính toán Reward / Stats\n",
    "        reward, covered_count = self._calculate_reward(self.agent_pos, self.target_pos)\n",
    "        \n",
    "        return self._get_state(), reward, covered_count\n",
    "\n",
    "    def _calculate_reward(self, a_pos, t_pos):\n",
    "        # Tính khoảng cách giữa tất cả Agent và tất cả Target\n",
    "        # a_pos: (1, N, 2), t_pos: (1, M, 2)\n",
    "        dist_matrix = torch.cdist(a_pos, t_pos, p=2) # (1, N, M)\n",
    "        \n",
    "        # Kiểm tra xem mỗi target có bị agent nào cover không (dist < R)\n",
    "        min_dist_per_target, _ = torch.min(dist_matrix, dim=1) # (1, M) - khoảng cách tới agent gần nhất\n",
    "        \n",
    "        is_covered = min_dist_per_target < self.coverage_radius\n",
    "        covered_count = is_covered.sum().item()\n",
    "        \n",
    "        # Reward = Tỷ lệ bao phủ\n",
    "        reward = covered_count / self.num_targets\n",
    "        \n",
    "        return reward, covered_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64a30e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_custom_loss(weights, agent_pos, target_pos):\n",
    "    \"\"\"\n",
    "    weights: (B, N, M) - Output từ model\n",
    "    agent_pos: (B, N, 2)\n",
    "    target_pos: (B, M, 2)\n",
    "    \"\"\"\n",
    "    # 1. Distance Cost: Khuyến khích chọn target ở gần\n",
    "    dist_matrix = torch.cdist(agent_pos, target_pos) # (B, N, M)\n",
    "    # Tổng khoảng cách có trọng số (Agent chọn target nào thì tính khoảng cách đó)\n",
    "    weighted_dist = (weights * dist_matrix).sum() \n",
    "    \n",
    "    # 2. Coverage / Diversity Cost: Khuyến khích bao phủ hết các target\n",
    "    # Cộng dồn trọng số theo cột (Targets). \n",
    "    # Nếu lý tưởng: mỗi target nhận được tổng weight = 1 (hoặc > 0).\n",
    "    target_coverage = weights.sum(dim=1) # (B, M)\n",
    "    \n",
    "    # Chúng ta muốn target_coverage càng đồng đều càng tốt.\n",
    "    # Phạt các target có coverage = 0.\n",
    "    # Sử dụng MSE so với mức lý tưởng (ví dụ: N/M hoặc đơn giản là > 0)\n",
    "    # Ở đây dùng hàm log barrier hoặc MSE loss để ép phân phối đều\n",
    "    diversity_loss = torch.sum((target_coverage - 1.0) ** 2) \n",
    "    \n",
    "    # Tổng hợp Loss\n",
    "    # Alpha điều chỉnh sự cân bằng giữa việc \"Chọn thằng gần nhất\" vs \"Chia nhau ra\"\n",
    "    loss = weighted_dist + 10.0 * diversity_loss \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu training...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 1. Forward Pass\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Lưu ý: Lúc train để temperature cao để khám phá, rồi giảm dần\u001b[39;00m\n\u001b[32m     26\u001b[39m temp = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m - episode / num_episodes) \n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m weights = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massignment_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgumbel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 2. Tính Loss (Dựa trên trạng thái hiện tại và quyết định của model)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Chúng ta muốn model đưa ra quyết định tối ưu CHO TRẠNG THÁI NÀY\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Lấy vị trí x,y để tính loss khoảng cách\u001b[39;00m\n\u001b[32m     32\u001b[39m curr_agent_pos = agent_states[:, :, :\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell_\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell_\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, agent_states, global_targets, assignment_mode, temperature)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent_states, global_targets, assignment_mode=\u001b[33m'\u001b[39m\u001b[33mgumbel\u001b[39m\u001b[33m'\u001b[39m, temperature=\u001b[32m1.0\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     batch_size, num_agents, _ = \u001b[43magent_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m()\n\u001b[32m     42\u001b[39m     _, num_targets, _ = global_targets.size()\n\u001b[32m     44\u001b[39m     agent_emb = \u001b[38;5;28mself\u001b[39m.agent_encoder(agent_states)      \u001b[38;5;66;03m# [B, N, Emb]\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Cấu hình\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_episodes = 500\n",
    "steps_per_episode = 50\n",
    "\n",
    "# Khởi tạo\n",
    "env = SimulationEnv(num_agents=4, num_targets=5, device=device)\n",
    "model = Attention(agent_state_dim=5, target_feature_dim=4).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_history = []\n",
    "reward_history = []\n",
    "\n",
    "print(\"Bắt đầu training...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset môi trường, lấy state đầu tiên\n",
    "    agent_states, global_targets = env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step in range(steps_per_episode):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        temp = max(0.5, 1.0 - episode / num_episodes)\n",
    "        \n",
    "        # Đảm bảo input là Tensor (phòng hờ)\n",
    "        if isinstance(agent_states, tuple): \n",
    "            agent_states = agent_states[0] # Fallback nếu vẫn bị lồng tuple\n",
    "            \n",
    "        weights = model(agent_states, global_targets, assignment_mode='gumbel', temperature=temp)\n",
    "        \n",
    "        # 2. Tính Loss\n",
    "        curr_agent_pos = agent_states[:, :, :2]\n",
    "        curr_target_pos = global_targets[:, :, :2]\n",
    "        \n",
    "        loss = compute_custom_loss(weights, curr_agent_pos, curr_target_pos)\n",
    "        \n",
    "        # 3. Backward & Update\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (Mẹo: Giúp train ổn định hơn, tránh gradient bùng nổ)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 4. Step Environment (QUAN TRỌNG: SỬA LỖI TẠI ĐÂY)\n",
    "        # env.step trả về: ((agents, targets), reward, count)\n",
    "        (next_agent_states, next_global_targets), reward, covered_count = env.step(weights.detach())\n",
    "        \n",
    "        # Cập nhật state cho vòng lặp sau\n",
    "        agent_states = next_agent_states\n",
    "        global_targets = next_global_targets\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Logging\n",
    "    avg_reward = total_reward / steps_per_episode\n",
    "    loss_history.append(total_loss)\n",
    "    reward_history.append(avg_reward)\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}: Loss = {total_loss:.2f}, Avg Coverage = {avg_reward*100:.1f}%\")\n",
    "\n",
    "print(\"Training hoàn tất!\")\n",
    "\n",
    "# Visualize kết quả training\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(reward_history)\n",
    "plt.title(\"Coverage Rate (%)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
